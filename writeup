The Basics
Lexical analysis or scanning is the process where the stream of characters making up the
source program is read from left-to-right and grouped into tokens. Tokens are sequences
of characters with a collective meaning. There are usually only a small number of tokens
for a programming language: constants (integer, double, char, string, etc.), operators
(arithmetic, relational, logical), punctuation, and reserved words.

The lexical analyzer takes a source program as input, and produces a stream of tokens as
output.

Lexemes:A lexeme is the actual character sequenceforming a token, the token is the general 
class that a lexeme belongs to. Some tokens have exactly one lexeme (e.g., the > character);
for others, there are many lexemes (e.g.,integer constants).


